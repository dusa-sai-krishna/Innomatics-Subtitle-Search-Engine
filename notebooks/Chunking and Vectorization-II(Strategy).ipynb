{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step- I Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pre-processed_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the.message.(1976).eng.1cd</td>\n",
       "      <td>name god gracious merci muhammad messeng god h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here.comes.the.grump.s01.e09.joltin.jack.in.bo...</td>\n",
       "      <td>ah princess dawn terri blooney looney soldier ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yumis.cells.s02.e13.episode.2.13.(2022).eng.1cd</td>\n",
       "      <td>yumi cell episod extrem polit yumi yumi get ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yumis.cells.s02.e14.episode.2.14.(2022).eng.1cd</td>\n",
       "      <td>yumi cell episod laptop first place mine mine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>broker.(2022).eng.1cd</td>\n",
       "      <td>go throw away give birth pleas take care anyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>recess.(1997).eng.1cd</td>\n",
       "      <td>bell ring children cheer wa umph ah burp right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>recess.(1997).eng.1cd</td>\n",
       "      <td>bell ring children cheer pop ah crash ah burp ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>recess.(1997).eng.1cd</td>\n",
       "      <td>bell ring cheer yell whimper org deprec pleas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>recess.(1997).eng.1cd</td>\n",
       "      <td>bell ring children cheer wha use free code joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>recess.(1997).eng.1cd</td>\n",
       "      <td>bell ring children cheer wha ah burp legal ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "index                                                      \n",
       "0                             the.message.(1976).eng.1cd   \n",
       "1      here.comes.the.grump.s01.e09.joltin.jack.in.bo...   \n",
       "2        yumis.cells.s02.e13.episode.2.13.(2022).eng.1cd   \n",
       "3        yumis.cells.s02.e14.episode.2.14.(2022).eng.1cd   \n",
       "4                                  broker.(2022).eng.1cd   \n",
       "...                                                  ...   \n",
       "9995                               recess.(1997).eng.1cd   \n",
       "9996                               recess.(1997).eng.1cd   \n",
       "9997                               recess.(1997).eng.1cd   \n",
       "9998                               recess.(1997).eng.1cd   \n",
       "9999                               recess.(1997).eng.1cd   \n",
       "\n",
       "                                   pre-processed_content  \n",
       "index                                                     \n",
       "0      name god gracious merci muhammad messeng god h...  \n",
       "1      ah princess dawn terri blooney looney soldier ...  \n",
       "2      yumi cell episod extrem polit yumi yumi get ma...  \n",
       "3      yumi cell episod laptop first place mine mine ...  \n",
       "4      go throw away give birth pleas take care anyth...  \n",
       "...                                                  ...  \n",
       "9995   bell ring children cheer wa umph ah burp right...  \n",
       "9996   bell ring children cheer pop ah crash ah burp ...  \n",
       "9997   bell ring cheer yell whimper org deprec pleas ...  \n",
       "9998   bell ring children cheer wha use free code joi...  \n",
       "9999   bell ring children cheer wha ah burp legal ten...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../data/0-10000eng_subtitles.csv',index_col=0).copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-II Perform Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two lists\n",
    "new_id=[]\n",
    "new_content=[]\n",
    "\n",
    "#loop through the dataframe and chunk the content\n",
    "def chunker(chunk_size,id,content):\n",
    "    \n",
    "    tokens=[token for token in content.split()]\n",
    "    #get length of the tokens\n",
    "    n=len(tokens)\n",
    "    \n",
    "    #initialize index and count\n",
    "    index,count=0,0\n",
    "    #print(id,content,tokens)\n",
    "    while True:\n",
    "        if index==0:\n",
    "            si,ei=index,256\n",
    "            if ei>n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index=ei\n",
    "        else:\n",
    "            si,ei=index-chunk_size,index+256-chunk_size #chunking buffer\n",
    "            if ei>n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index=ei\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-0</td>\n",
       "      <td>name god gracious merci muhammad messeng god h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-1</td>\n",
       "      <td>abu sofyan revel song begin abu sofyan invit p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-2</td>\n",
       "      <td>restrain nephew divid citi heart hous divid ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-3</td>\n",
       "      <td>muhammad generous yes give share pass man with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-4</td>\n",
       "      <td>lash face teach mouth lesson whip whip cut whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116920</th>\n",
       "      <td>9999-2</td>\n",
       "      <td>candi spa guess got minut well thank ladi migh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116921</th>\n",
       "      <td>9999-3</td>\n",
       "      <td>fallen trap door yeah gotten strap laboratori ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116922</th>\n",
       "      <td>9999-4</td>\n",
       "      <td>tell futur hold besid rainbow happi man would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116923</th>\n",
       "      <td>9999-5</td>\n",
       "      <td>dinosaur earthquak gold mine believ clearanc t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116924</th>\n",
       "      <td>9999-6</td>\n",
       "      <td>got one hope left hey el mike grand talk stop ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116925 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                            content\n",
       "0          0-0  name god gracious merci muhammad messeng god h...\n",
       "1          0-1  abu sofyan revel song begin abu sofyan invit p...\n",
       "2          0-2  restrain nephew divid citi heart hous divid ge...\n",
       "3          0-3  muhammad generous yes give share pass man with...\n",
       "4          0-4  lash face teach mouth lesson whip whip cut whi...\n",
       "...        ...                                                ...\n",
       "116920  9999-2  candi spa guess got minut well thank ladi migh...\n",
       "116921  9999-3  fallen trap door yeah gotten strap laboratori ...\n",
       "116922  9999-4  tell futur hold besid rainbow happi man would ...\n",
       "116923  9999-5  dinosaur earthquak gold mine believ clearanc t...\n",
       "116924  9999-6  got one hope left hey el mike grand talk stop ...\n",
       "\n",
       "[116925 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    chunker(25,i,df['pre-processed_content'][i])\n",
    "\n",
    "new_df=pd.DataFrame({'id':new_id,'content':new_content})\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-III Send to ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chromadb\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a persistent client\n",
    "client = chromadb.PersistentClient(path=\"../data/chromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection=client.get_or_create_collection(name='demo',metadata={\"hnsw:space\": \"cosine\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(documents=new_df['content'].to_list()[:10],ids=new_df['id'].to_list()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import inflect\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "def text_preprocessing(corpus, flag):\n",
    "    # Compiled regular expressions for patterns\n",
    "    pattern1 = re.compile(r'\\d+\\r\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}')\n",
    "    pattern2 = re.compile(r'\\r\\nWatch any video online with Open-SUBTITLES\\r\\nFree Browser extension: osdb.link/ext\\r\\n\\r\\n\\r\\n')\n",
    "    pattern3 = re.compile(r'Please rate this subtitle at www.osdb.link/agwma\\r\\nHelp other users to choose the best subtitles')\n",
    "    pattern4 = re.compile(r'(\\r\\n)+')\n",
    "    pattern5 = re.compile(r'<[/]?\\w+>')\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove timestamps\n",
    "    corpus = re.sub(pattern1, '', corpus)\n",
    "\n",
    "    # remov header and footer\n",
    "    corpus = re.sub(pattern2, '', corpus)\n",
    "    corpus = re.sub(pattern3, '', corpus)\n",
    "\n",
    "    # remove extra line breaks\n",
    "    corpus = re.sub(pattern4, '\\r\\n', corpus)\n",
    "\n",
    "    # remove html tags\n",
    "    corpus = re.sub(pattern5, '', corpus)\n",
    "\n",
    "    # change  of numbers\n",
    "    #p = inflect.engine()\n",
    "    #corpus = re.sub(r'\\d+', lambda x: p.number_to_words(x.group(0)), corpus)\n",
    "\n",
    "    # remove special characters\n",
    "    corpus = re.sub('[^a-zA-Z]', ' ', corpus)\n",
    "\n",
    "    # convert to lower case\n",
    "    corpus = corpus.lower()\n",
    "\n",
    "    # removal of whitespaces\n",
    "    corpus = ' '.join(corpus.split())\n",
    "\n",
    "    # tokenize\n",
    "    words = word_tokenize(corpus)\n",
    "\n",
    "    # Stemming or Lemmatization\n",
    "    if flag == \"stemming\":\n",
    "        # stemming\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        return ' '.join(stemmer.stem(word) for word in words if word not in stop_words)\n",
    "    else:\n",
    "        # lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return ' '.join(lemmatizer.lemmatize(word) for word in words if word not in stop_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function where user query returns top 10 relevant searches\n",
    "def getResults(query,flag):\n",
    "    \n",
    "    # Pre process the query\n",
    "    text=text_preprocessing(query, flag)\n",
    "    print(text)\n",
    "    \n",
    "    #query with Chroma DB\n",
    "    results = collection.query(\n",
    "        query_texts=[text],\n",
    "        n_results=20\n",
    "    )\n",
    "    \n",
    "    #get distinct-parent ids\n",
    "    ids=results['ids'][0]\n",
    "    distinct_ids=set()\n",
    "    for i in ids:\n",
    "        distinct_ids.add(i.split('-')[0])\n",
    "    \n",
    "    #query these distinct ids\n",
    "    count=0\n",
    "    results=[]\n",
    "    for ind in distinct_ids:\n",
    "        if count>11: break\n",
    "        results.append(df.loc[int(ind),'name'])\n",
    "        count+=1\n",
    "    \n",
    "    return distinct_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getResults('Oh! God pls help me','stemming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-IV Create a pipeline\n",
    "- Let's create a pipeline which would take a 10,000 row dataframe of pre-processed transcripts\n",
    "- Chunk them and simultaneously upload to the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "#create a persistentClient\n",
    "client=chromadb.PersistentClient(path=\"../data/chromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def new_chunker(chunk_size, id, content,collection,new_id,new_content):\n",
    "    \n",
    "    tokens = [token for token in content.split()]\n",
    "    # Get length of the tokens\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Initialize index and count\n",
    "    index, count = 0, 0\n",
    "    # Print(id,content,tokens)\n",
    "    while True:\n",
    "        if index == 0:\n",
    "            si, ei = index,256\n",
    "            if ei > n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index = ei\n",
    "        else:\n",
    "            si, ei = index - chunk_size, index + 256 - chunk_size  # chunking buffer\n",
    "            if ei > n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index = ei\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chunker_Uploader(chunk_size,collection_name,file_path):\n",
    "    \n",
    "    #Data Ingestion\n",
    "    df=pd.read_csv(file_path,index_col=0)\n",
    "    #create or get a collection\n",
    "    collection=client.get_or_create_collection(name=collection_name,metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    #create two lists\n",
    "    new_id=[]\n",
    "    new_content=[]\n",
    "    \n",
    "    #chunk and upload\n",
    "    for i in df.index:\n",
    "        new_chunker(chunk_size,i,df.loc[i,'pre-processed_content'],collection,new_id,new_content)\n",
    "        \n",
    "      # Initialize tqdm\n",
    "    progress_bar = tqdm(total=len(new_id), desc=\"Adding vectors\")\n",
    "    \n",
    "    # Add chunks to the collection\n",
    "    for i, (new_doc_id, new_doc_content) in enumerate(zip(new_id, new_content)):\n",
    "        # Add document to the collection\n",
    "        collection.add(ids=new_doc_id, documents=new_doc_content)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "   \n",
    "    print('Vectors are added!!!')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1125/1125 [00:45<00:00, 24.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors are added!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Chunker_Uploader(25,'transcripts','../data/0-10000eng_subtitles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- The above chunk_uploader uploads entire 1 lakh plus vectors in a single pass.\n",
    "- But the ChromaDB suggest that in a single pass a maximum of 1 Lakh vectors can be added.Hence need to modify it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-V Use the pipeline to add vectors to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "#create a persistentClient\n",
    "client=chromadb.PersistentClient(path=\"../data/chromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def new_chunker(chunk_size, id, content,collection,new_id,new_content):\n",
    "    \n",
    "    tokens = [token for token in content.split()]\n",
    "    # Get length of the tokens\n",
    "    n = len(tokens)\n",
    "    \n",
    "    # Initialize index and count\n",
    "    index, count = 0, 0\n",
    "    # Print(id,content,tokens)\n",
    "    while True:\n",
    "        if index == 0:\n",
    "            si, ei = index, 256\n",
    "            if ei > n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index = ei\n",
    "        else:\n",
    "            si, ei = index - chunk_size, index + 256 - chunk_size  # chunking buffer\n",
    "            if ei > n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index = ei\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Chunker_Uploader1(chunk_size,collection_name,file_path):\n",
    "    \n",
    "    #Data Ingestion\n",
    "    df=pd.read_csv(file_path,index_col=0)\n",
    "    df=df.iloc[:11000,:]\n",
    "    #create or get a collection\n",
    "    collection=client.get_or_create_collection(name=collection_name,metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    #create two lists\n",
    "    new_id=[]\n",
    "    new_content=[]\n",
    "    print('Chunking Began')\n",
    "    #chunk and upload\n",
    "    for i in df.index:\n",
    "        new_chunker(chunk_size,i,df.loc[i,'pre-processed_content'],collection,new_id,new_content)\n",
    "    \n",
    "    print('Chunking Completed')\n",
    "    \n",
    "    pass_size = 10000\n",
    "    for i in range(0, len(new_id), pass_size):\n",
    "        si = i\n",
    "        ei = min(si + pass_size, len(new_id))  # Ensure ei does not exceed the list length\n",
    "        curr_id = new_id[si:ei]\n",
    "        curr_content = new_content[si:ei]\n",
    "        \n",
    "        # Initialize tqdm\n",
    "        progress_bar = tqdm(total=len(new_id), desc=\"Adding vectors\")\n",
    "        \n",
    "        # Add chunks to the collection\n",
    "        for i, (new_doc_id, new_doc_content) in enumerate(zip(curr_id, curr_content)):\n",
    "            # Add document to the collection\n",
    "            collection.add(ids=new_doc_id, documents=new_doc_content)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "    \n",
    "        print('Vectors are added!!!')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def Chunker_Uploader(chunk_size, collection_name, file_path):\n",
    "    # Data Ingestion\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "    df = df.iloc[:11000, :]  # Limiting the dataframe for demonstration\n",
    "\n",
    "    # Create or get a collection\n",
    "    collection = client.get_or_create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    # Create two lists\n",
    "    new_id = []\n",
    "    new_content = []\n",
    "\n",
    "    print('Chunking Began')\n",
    "    # Chunk and upload\n",
    "    for i in df.index:\n",
    "        new_chunker(chunk_size, i, df.loc[i, 'pre-processed_content'], collection, new_id, new_content)\n",
    "\n",
    "    print('Chunking Completed')\n",
    "    pass_size = 10000\n",
    "    for i in range(0, len(new_id), pass_size):\n",
    "        si = i\n",
    "        ei = min(si + pass_size, len(new_id))  # Ensure ei does not exceed the list length\n",
    "        curr_id = new_id[si:ei]\n",
    "        curr_content = new_content[si:ei]\n",
    "\n",
    "        # Initialize tqdm with chunk size\n",
    "        progress_bar = tqdm(total=len(curr_id), desc=\"Adding vectors\", unit=\"doc\")\n",
    "\n",
    "        # Add chunks to the collection\n",
    "        for new_doc_id, new_doc_content in zip(curr_id, curr_content):\n",
    "            # Add document to the collection\n",
    "            collection.add(ids=new_doc_id, documents=new_doc_content)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "    print('Vectors are added!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chunker_Uploader1(25,'demo','../data/Pre-processed_content/0-10000eng_subtitles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection('demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6239"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection=client.get_or_create_collection('transcripts')\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- Chunks are being uploaded to the Chroma DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
