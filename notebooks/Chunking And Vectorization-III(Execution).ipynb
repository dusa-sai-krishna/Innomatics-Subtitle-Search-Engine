{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2301c694-bbf6-4f24-a7f9-37b43e5e4a04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 44.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[K     |████████████████████████████████| 773 kB 58.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nltk) (4.66.2)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.0 nltk-3.8.1 regex-2023.12.25\n"
     ]
    }
   ],
   "source": [
    "#pip install chromadb\n",
    "#!pip install pandas\n",
    "#!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e66b0-d18d-43e9-a30a-1ce97a8d8ee6",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2265ddb2-ae3d-48a5-aac6-4f9c02d89141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42a3e51-cf54-45f6-b9d0-c4d7a550bf1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a persistent chroma db\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "#create a persistentClient\n",
    "client=chromadb.PersistentClient(path=\"ChromaDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041a9d2c-37b2-4627-acea-00c4df84df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to chunk\n",
    "\n",
    "\n",
    "def new_chunker(chunk_size, id, content,collection,new_id,new_content):\n",
    "    \n",
    "    tokens = [token for token in content.split()]\n",
    "    # Get length of the tokens\n",
    "    n = len(tokens)\n",
    "\n",
    "    # Initialize index and count\n",
    "    index, count = 0, 0\n",
    "    # Print(id,content,tokens)\n",
    "    while True:\n",
    "        if index == 0:\n",
    "            si, ei = index, 256\n",
    "            if ei > n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index = ei\n",
    "        else:\n",
    "            si, ei = index - chunk_size, index + 256 - chunk_size  # chunking buffer\n",
    "            if ei > n:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:]))\n",
    "                break\n",
    "            else:\n",
    "                new_id.append(f\"{id}-{count}\")\n",
    "                new_content.append(\" \".join(tokens[si:ei]))\n",
    "                index = ei\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b446e7f-cc93-4784-8ef1-5e513b1cef43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "\n",
    "def Chunker_Uploader(chunk_size, collection_name, file_path):\n",
    "\n",
    "    # Data Ingestion\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    \n",
    "    # Create or get a collection\n",
    "    collection = client.get_or_create_collection(name=collection_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "    # Create two lists\n",
    "    new_id = []\n",
    "    new_content = []\n",
    "    print('Chunking Started!')\n",
    "    # Chunk and upload\n",
    "    for i in df.index:\n",
    "        if type(df.loc[i,'pre-processed_content'])==str:\n",
    "            new_chunker(chunk_size, i, df.loc[i, 'pre-processed_content'], collection, new_id, new_content)\n",
    "        \n",
    "    print('Chunking Ended',i)\n",
    "    pass_size = 10000\n",
    "    passes=ceil(len(new_id)/10000)\n",
    "    print('No of passes',passes)\n",
    "    for i in range(passes):\n",
    "        print('Current Pass no:',i+1)\n",
    "        si = 0+(i*pass_size)\n",
    "        ei = min(si + pass_size, len(new_id) ) # Ensure ei does not exceed the list length\n",
    "        \n",
    "        curr_id = new_id[si:ei]\n",
    "        curr_content = new_content[si:ei]\n",
    "        # Initialize tqdm\n",
    "        progress_bar = tqdm(total=len(curr_id), desc=\"Adding vectors\", unit=\"doc\")\n",
    "\n",
    "        # Add chunks to the collection\n",
    "        for new_doc_id, new_doc_content in zip(curr_id, curr_content):\n",
    "            # Add document to the collection\n",
    "            collection.add(ids=new_doc_id, documents=new_doc_content)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        print('Vectors are added!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7872edcb-d0d1-4cee-b4f5-157c11c890d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Started!\n",
      "Chunking Ended 79999\n",
      "No of passes 13\n",
      "Current Pass no: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2bc86e8ec44a919ab962291ebba690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding vectors:   0%|          | 0/10000 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 71626-8\n",
      "Add of existing embedding ID: 71626-8\n",
      "Insert of existing embedding ID: 71626-9\n",
      "Add of existing embedding ID: 71626-9\n",
      "Insert of existing embedding ID: 71626-10\n",
      "Add of existing embedding ID: 71626-10\n",
      "Insert of existing embedding ID: 71627-0\n",
      "Add of existing embedding ID: 71627-0\n",
      "Insert of existing embedding ID: 71627-1\n",
      "Add of existing embedding ID: 71627-1\n",
      "Insert of existing embedding ID: 71627-2\n",
      "Add of existing embedding ID: 71627-2\n",
      "Insert of existing embedding ID: 71627-3\n",
      "Add of existing embedding ID: 71627-3\n",
      "Insert of existing embedding ID: 71627-4\n",
      "Add of existing embedding ID: 71627-4\n",
      "Insert of existing embedding ID: 71627-5\n",
      "Add of existing embedding ID: 71627-5\n",
      "Insert of existing embedding ID: 71627-6\n",
      "Add of existing embedding ID: 71627-6\n",
      "Insert of existing embedding ID: 71627-7\n",
      "Add of existing embedding ID: 71627-7\n",
      "Insert of existing embedding ID: 71627-8\n",
      "Add of existing embedding ID: 71627-8\n",
      "Insert of existing embedding ID: 71627-9\n",
      "Add of existing embedding ID: 71627-9\n",
      "Insert of existing embedding ID: 71627-10\n",
      "Add of existing embedding ID: 71627-10\n",
      "Insert of existing embedding ID: 71627-11\n",
      "Add of existing embedding ID: 71627-11\n",
      "Insert of existing embedding ID: 71628-0\n",
      "Add of existing embedding ID: 71628-0\n",
      "Insert of existing embedding ID: 71628-1\n",
      "Add of existing embedding ID: 71628-1\n",
      "Insert of existing embedding ID: 71628-2\n",
      "Add of existing embedding ID: 71628-2\n",
      "Insert of existing embedding ID: 71628-3\n",
      "Add of existing embedding ID: 71628-3\n",
      "Insert of existing embedding ID: 71628-4\n",
      "Add of existing embedding ID: 71628-4\n",
      "Insert of existing embedding ID: 71628-5\n",
      "Add of existing embedding ID: 71628-5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_944/3671994065.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mChunker_Uploader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'transcripts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"70000-80000eng_subtitles.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_944/1317081541.py\u001b[0m in \u001b[0;36mChunker_Uploader\u001b[0;34m(chunk_size, collection_name, file_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_doc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_doc_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Add document to the collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_doc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_doc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Update progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# At this point, we know that one of documents or images are provided from the validation above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36m_embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;34m\"https://docs.trychroma.com/embeddings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             )\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEmbeddingFunction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalidate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_cast_one_to_many_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/chromadb/utils/embedding_functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# Only download the model when it is actually used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_model_if_not_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_download_model_if_not_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/chromadb/utils/embedding_functions.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, documents, batch_size)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 ),\n\u001b[1;32m    467\u001b[0m             }\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0mlast_hidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;31m# Perform mean pooling with attention weighting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Chunker_Uploader(50,'transcripts',\"70000-80000eng_subtitles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625e0b6-9b95-485a-b3c1-10f92d3e5739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Chunker_Uploader(0,'demo',\"10000-20000eng_subtitles.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
