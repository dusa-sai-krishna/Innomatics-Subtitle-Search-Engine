{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1 Connect to Chroma Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2 Create a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.create_collection(name=\"demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3 Add some text documents to the Collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chroma will store your text, and handle tokenization, embedding, and indexing automatically\n",
    "- If you have already generated embeddings yourself, you can load them directly in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: id1\n",
      "Insert of existing embedding ID: id2\n",
      "Add of existing embedding ID: id1\n",
      "Add of existing embedding ID: id2\n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=[\"This is a document\", \"This is another document\",\"This is not a document\"],\n",
    "    metadatas=[{\"source\": \"my_source\"}, {\"source\": \"my_source\"},{\"source\": \"my_source\"}],\n",
    "    ids=[\"id1\", \"id2\",\"id3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.add(\n",
    "    #embeddings=[[1.2, 2.3, 4.5], [6.7, 8.2, 9.2]],\n",
    "    #documents=[\"This is a document\", \"This is another document\",\"this is not a document\"],\n",
    "    #metadatas=[{\"source\": \"my_source\"}, {\"source\": \"my_source\"},{\"source\": \"my_source\"}],\n",
    "    #ids=[\"id1\", \"id2\",\"id3\"]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id1', 'id3']],\n",
       " 'distances': [[0.7111214399337769, 0.8618925213813782]],\n",
       " 'metadatas': [[{'source': 'my_source'}, {'source': 'my_source'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['This is a document', 'This is not a document']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document\"],\n",
    "    n_results=2\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a persistent chroma db\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "#create a persistentClient\n",
    "client=chromadb.PersistentClient(path=r\"C:\\Users\\dsai9\\Projects\\Semantic Search Engine\\data\\ChromaDB_8000+_bkp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dsai9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dsai9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dsai9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_preprocessing(corpus, flag):\n",
    "    # Compiled regular expressions for patterns\n",
    "    pattern1 = re.compile(r'\\d+\\r\\n\\d{2}:\\d{2}:\\d{2},\\d{3} --> \\d{2}:\\d{2}:\\d{2},\\d{3}')\n",
    "    pattern2 = re.compile(r'\\r\\nWatch any video online with Open-SUBTITLES\\r\\nFree Browser extension: osdb.link/ext\\r\\n\\r\\n\\r\\n')\n",
    "    pattern3 = re.compile(r'Please rate this subtitle at www.osdb.link/agwma\\r\\nHelp other users to choose the best subtitles')\n",
    "    pattern4 = re.compile(r'(\\r\\n)+')\n",
    "    pattern5 = re.compile(r'<[/]?\\w+>')\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # remove timestamps\n",
    "    corpus = re.sub(pattern1, '', corpus)\n",
    "\n",
    "    # remov header and footer\n",
    "    corpus = re.sub(pattern2, '', corpus)\n",
    "    corpus = re.sub(pattern3, '', corpus)\n",
    "\n",
    "    # remove extra line breaks\n",
    "    corpus = re.sub(pattern4, '\\r\\n', corpus)\n",
    "\n",
    "    # remove html tags\n",
    "    corpus = re.sub(pattern5, '', corpus)\n",
    "\n",
    "    # change  of numbers\n",
    "    #p = inflect.engine()\n",
    "    #corpus = re.sub(r'\\d+', lambda x: p.number_to_words(x.group(0)), corpus)\n",
    "\n",
    "    # remove special characters\n",
    "    corpus = re.sub('[^a-zA-Z]', ' ', corpus)\n",
    "\n",
    "    # convert to lower case\n",
    "    corpus = corpus.lower()\n",
    "\n",
    "    # removal of whitespaces\n",
    "    corpus = ' '.join(corpus.split())\n",
    "\n",
    "    # tokenize\n",
    "    words = word_tokenize(corpus)\n",
    "\n",
    "    # Stemming or Lemmatization\n",
    "    if flag == \"stemming\":\n",
    "        # stemming\n",
    "        stemmer = SnowballStemmer(language='english')\n",
    "        return ' '.join(stemmer.stem(word) for word in words if word not in stop_words)\n",
    "    else:\n",
    "        # lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return ' '.join(lemmatizer.lemmatize(word) for word in words if word not in stop_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function where user query returns top 10 relevant searches\n",
    "def getResults(query,flag):\n",
    "    \n",
    "    # Pre process the query\n",
    "    text=text_preprocessing(query, flag)\n",
    "    print(text)\n",
    "    \n",
    "      # Create or get a collection\n",
    "    collection = client.get_or_create_collection(name='transcripts', metadata={\"hnsw:space\": \"cosine\"})\n",
    "    #query with Chroma DB\n",
    "    results = collection.query(\n",
    "        query_texts=[text],\n",
    "        n_results=30\n",
    "    )\n",
    "    \n",
    "    #get distinct-parent ids\n",
    "    ids=results['ids'][0]\n",
    "    distinct_ids=set()\n",
    "    for i in ids:\n",
    "        distinct_ids.add(i.split('-')[0])\n",
    "    print(distinct_ids)\n",
    "    \n",
    "    #query these distinct ids\n",
    "    count=0\n",
    "    results=[]\n",
    "    for ind in distinct_ids:\n",
    "        if count>20: break\n",
    "        #results.append(df.loc[int(ind),'name'])\n",
    "        #count+=1\n",
    "    \n",
    "    return distinct_ids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey god pls help\n",
      "{'80479', '81339', '81786', '80394', '81550', '80177', '80158', '81380', '80159', '80076', '80033', '82111', '81001', '80138', '81430', '81074', '80679', '82057', '81631', '81907', '80355', '81653', '81586', '81622', '80247', '80366', '80574', '81431', '80228', '80560'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'80033',\n",
       " '80076',\n",
       " '80138',\n",
       " '80158',\n",
       " '80159',\n",
       " '80177',\n",
       " '80228',\n",
       " '80247',\n",
       " '80355',\n",
       " '80366',\n",
       " '80394',\n",
       " '80479',\n",
       " '80560',\n",
       " '80574',\n",
       " '80679',\n",
       " '81001',\n",
       " '81074',\n",
       " '81339',\n",
       " '81380',\n",
       " '81430',\n",
       " '81431',\n",
       " '81550',\n",
       " '81586',\n",
       " '81622',\n",
       " '81631',\n",
       " '81653',\n",
       " '81786',\n",
       " '81907',\n",
       " '82057',\n",
       " '82111'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResults('Hey God!! Pls help me','stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pre-processed_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60000</th>\n",
       "      <td>black.sunday.(1977).eng.1cd</td>\n",
       "      <td>use free code joinnow attent pleas passeng fli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60001</th>\n",
       "      <td>hapritza.hagdola.(1970).eng.1cd</td>\n",
       "      <td>gentlemen get eas way right two still interrog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60002</th>\n",
       "      <td>witness.in.the.war.zone.(1987).eng.1cd</td>\n",
       "      <td>org deprec pleas implement rest api hello hell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60003</th>\n",
       "      <td>disengagement.(2007).eng.1cd</td>\n",
       "      <td>cigarett happen french dutch paper prefer mm f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60004</th>\n",
       "      <td>the.romantics.s01.e01.the.boy.from.jalandhar.(...</td>\n",
       "      <td>katrina kaif hi nice meet lar hi interview hi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>sky.high.s01.e07.en.la.boca.del.lobo.(2023).en...</td>\n",
       "      <td>advertis product brand contact today sky high ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>sky.high.s01.e01.los.vivos.y.los.muertos.(2023...</td>\n",
       "      <td>inspir real event charact fiction dramat purpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>sky.high.s01.e02.nuevas.amistades.viejos.enemi...</td>\n",
       "      <td>sky high seri million famili carri heart rip c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>sky.high.s01.e03.cuentas.pendientes.(2023).eng...</td>\n",
       "      <td>sky high seri post offic offic manag deliveri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>sky.high.s01.e04.caballo.de.troya.(2023).eng.1cd</td>\n",
       "      <td>advertis product brand contact today sky high ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    name  \\\n",
       "index                                                      \n",
       "60000                        black.sunday.(1977).eng.1cd   \n",
       "60001                    hapritza.hagdola.(1970).eng.1cd   \n",
       "60002             witness.in.the.war.zone.(1987).eng.1cd   \n",
       "60003                       disengagement.(2007).eng.1cd   \n",
       "60004  the.romantics.s01.e01.the.boy.from.jalandhar.(...   \n",
       "...                                                  ...   \n",
       "69995  sky.high.s01.e07.en.la.boca.del.lobo.(2023).en...   \n",
       "69996  sky.high.s01.e01.los.vivos.y.los.muertos.(2023...   \n",
       "69997  sky.high.s01.e02.nuevas.amistades.viejos.enemi...   \n",
       "69998  sky.high.s01.e03.cuentas.pendientes.(2023).eng...   \n",
       "69999   sky.high.s01.e04.caballo.de.troya.(2023).eng.1cd   \n",
       "\n",
       "                                   pre-processed_content  \n",
       "index                                                     \n",
       "60000  use free code joinnow attent pleas passeng fli...  \n",
       "60001  gentlemen get eas way right two still interrog...  \n",
       "60002  org deprec pleas implement rest api hello hell...  \n",
       "60003  cigarett happen french dutch paper prefer mm f...  \n",
       "60004  katrina kaif hi nice meet lar hi interview hi ...  \n",
       "...                                                  ...  \n",
       "69995  advertis product brand contact today sky high ...  \n",
       "69996  inspir real event charact fiction dramat purpo...  \n",
       "69997  sky high seri million famili carri heart rip c...  \n",
       "69998  sky high seri post offic offic manag deliveri ...  \n",
       "69999  advertis product brand contact today sky high ...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/Pre-processed_content/50000-60000eng_subtitles.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
